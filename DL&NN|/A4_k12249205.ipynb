{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {},
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {},
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e., \n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units. \n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself. \n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d28afb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Q = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.K = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.V = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        N, T, D = x.shape\n",
    "        Q = x @ self.Q \n",
    "        K = x @ self.K   \n",
    "        V = x @ self.V\n",
    "       \n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(D)\n",
    "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        Y = attention_weights @ V \n",
    "\n",
    "        return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {},
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise. \n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers. \n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel. \n",
    "Apply the first dropout layer direcly after the softmax. \n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input. \n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$. \n",
    "Finally, apply the second dropout layer after the output projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ddee2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(MultiHeadCausalSelfAttention, self).__init__()\n",
    "        assert hidden_size % n_head == 0, \"hidden_size must be divisible by n_head\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = hidden_size // n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Q = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.K = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.V = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.W_O = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        N, T, D = x.shape\n",
    "        q = x @ self.Q\n",
    "        k = x @ self.K\n",
    "        v = x @ self.V\n",
    "        Q = q.view(N, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        K = k.view(N, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        V = v.view(N, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "       # print(\"Q.transpose(1, 2):\",Q.transpose(1, 2).shape)\n",
    "        #print(\"K.transpose(1, 3):\",K.transpose(1, 3).shape)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "        att_w = F.softmax(scores, dim=-1)\n",
    "        att_w = self.dropout(att_w)\n",
    "        Y = att_w @ V\n",
    "        Y = Y.transpose(1, 2).contiguous().view(N, T, D)\n",
    "        Y = Y @ self.W_O\n",
    "        Y = self.dropout(Y)\n",
    "\n",
    "        return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`. \n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs. \n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7ca16758",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.f1 = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        self.f2 = nn.Linear(4 * hidden_size, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dpt = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f1(x)  \n",
    "        x = self.gelu(x) \n",
    "        x = self.dpt(x)  \n",
    "        x = self.f2(x)  \n",
    "        x = self.dpt(x)  \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {},
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`. \n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b7b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "        mlp = MLP(hidden_size, dropout)\n",
    "        x = x + att(self.ln1(x))\n",
    "        x = x + mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {},
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`. \n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple. \n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence. \n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs. \n",
    "Add the two embeddings and apply a dropout layer. \n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`. \n",
    "Finally, apply the cross-entropy loss function to the logits. \n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights. \n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero. \n",
    "Use the argument `dropout` as intensity for all dropout layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cbe14a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([4, 128, 10000])\n",
      "Loss: 9.222928047180176\n"
     ]
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embed = nn.Embedding(context_size, hidden_size)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.fc.weight = self.token_embed.weight\n",
    "        self._init_weights(n_layer)\n",
    "\n",
    "    def _init_weights(self, n_layer):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(2 * n_layer))\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        N, T = x.shape\n",
    "        token_embeds = self.token_embed(x)  \n",
    "        position_embeds = self.position_embed(torch.arange(T, device=x.device))  \n",
    "        position_embeds = position_embeds.unsqueeze(0).expand(N, T, -1)\n",
    "        x = self.dropout(token_embeds + position_embeds)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        #x = nn.LayerNorm(hidden_size)\n",
    "        logs = self.fc(x)\n",
    "        #logs = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        loss = F.cross_entropy(logs.view(-1, logs.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logs, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {},
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`. \n",
    "Divide the model parameters into two groups. \n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`. \n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7e8be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "\n",
    "    for name, param in self.named_parameters():\n",
    "        if param.ndim >= 2: \n",
    "            decay_params.append(param)\n",
    "        else:  \n",
    "            no_decay_params.append(param)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([{\"params\": decay_params, \"weight_decay\": weight_decay}, {\"params\": no_decay_params, \"weight_decay\": 0.0}, ], lr=learning_rate, betas=betas)\n",
    "    return optimizer\n",
    "\n",
    "GPT.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {},
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that \n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "641a76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/500, Loss: 0.0923\n",
      "[0] Train Loss: 3.6924, Val Loss: 3.6941\n",
      "best model saved with val loss: 3.6941\n",
      "Iter 10/500, Loss: 0.0923\n",
      "Iter 20/500, Loss: 0.0923\n",
      "Iter 30/500, Loss: 0.0922\n",
      "Iter 40/500, Loss: 0.0921\n",
      "Iter 50/500, Loss: 0.0920\n",
      "Iter 60/500, Loss: 0.0913\n",
      "Iter 70/500, Loss: 0.0909\n",
      "Iter 80/500, Loss: 0.0893\n",
      "Iter 90/500, Loss: 0.0858\n",
      "Iter 100/500, Loss: 0.0824\n",
      "Iter 110/500, Loss: 0.0799\n",
      "Iter 120/500, Loss: 0.0770\n",
      "Iter 130/500, Loss: 0.0748\n",
      "Iter 140/500, Loss: 0.0745\n",
      "Iter 150/500, Loss: 0.0743\n",
      "Iter 160/500, Loss: 0.0738\n",
      "Iter 170/500, Loss: 0.0734\n",
      "Iter 180/500, Loss: 0.0732\n",
      "Iter 190/500, Loss: 0.0734\n",
      "Iter 200/500, Loss: 0.0735\n",
      "Iter 210/500, Loss: 0.0731\n",
      "Iter 220/500, Loss: 0.0729\n",
      "Iter 230/500, Loss: 0.0730\n",
      "Iter 240/500, Loss: 0.0730\n",
      "Iter 250/500, Loss: 0.0728\n",
      "[250] Train Loss: 2.9159, Val Loss: 2.9280\n",
      "best model saved with val loss: 2.9280\n",
      "Iter 260/500, Loss: 0.0733\n",
      "Iter 270/500, Loss: 0.0730\n",
      "Iter 280/500, Loss: 0.0729\n",
      "Iter 290/500, Loss: 0.0730\n",
      "Iter 300/500, Loss: 0.0727\n",
      "Iter 310/500, Loss: 0.0726\n",
      "Iter 320/500, Loss: 0.0726\n",
      "Iter 330/500, Loss: 0.0729\n",
      "Iter 340/500, Loss: 0.0725\n",
      "Iter 350/500, Loss: 0.0730\n",
      "Iter 360/500, Loss: 0.0729\n",
      "Iter 370/500, Loss: 0.0730\n",
      "Iter 380/500, Loss: 0.0726\n",
      "Iter 390/500, Loss: 0.0728\n",
      "Iter 400/500, Loss: 0.0725\n",
      "Iter 410/500, Loss: 0.0729\n",
      "Iter 420/500, Loss: 0.0728\n",
      "Iter 430/500, Loss: 0.0727\n",
      "Iter 440/500, Loss: 0.0729\n",
      "Iter 450/500, Loss: 0.0727\n",
      "Iter 460/500, Loss: 0.0725\n",
      "Iter 470/500, Loss: 0.0727\n",
      "Iter 480/500, Loss: 0.0725\n",
      "Iter 490/500, Loss: 0.0726\n",
      "Training complete.\n",
      "Best validation loss: 2.9280\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 250 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger training batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 64 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 4 # number of layers\n",
    "n_head = 4 # number of attention heads\n",
    "hidden_size = 128 # layer size\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 500 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.99 # for AdamW\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "min_lr = 1e-4 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "    \n",
    "    with open(f'trump_{split}.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long)\n",
    "    return text\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc. \n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head, dropout)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    comb_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step in range(gradient_accumulation_steps):\n",
    "        X, Y = get_batch('train')\n",
    "       # X, Y = X.cuda(), Y.cuda()\n",
    "      #  model = model.cuda()\n",
    "        logs, loss = model(X, Y)\n",
    "        loss = loss / gradient_accumulation_steps \n",
    "        loss.backward()\n",
    "        comb_loss += loss.item()\n",
    "    if grad_clip > 0.0:\n",
    "        par = model.parameters()\n",
    "        torch.nn.utils.clip_grad_norm_(par, grad_clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % log_interval == 0:\n",
    "        mean_loss = comb_loss / gradient_accumulation_steps\n",
    "      #  print(f\"Iter {iter_num}/{max_iters}, Loss: {mean_loss:.4f}, LR: {lr:.6e}\")\n",
    "        print(f\"Iter {iter_num}/{max_iters}, Loss: {mean_loss:.4f}\")\n",
    "\n",
    "    if iter_num % eval_interval == 0:\n",
    "        eval_loss = estimate_loss()\n",
    "        #print(f\"[{iter_num] Train Loss: {mean_loss['train']:.4f}, Val Loss: {mean_loss['val']:.4f}\")\n",
    "        print(f\"[{iter_num}] Train Loss: {eval_loss['train']:.4f}, Val Loss: {eval_loss['val']:.4f}\")\n",
    "\n",
    "        if eval_loss['val'] < best_val_loss:\n",
    "            best_val_loss = eval_loss['val']\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f\"best model saved with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {},
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`. \n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model. \n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax. \n",
    "After applying the softmax, sample the next token from the resulting categorical distribution. \n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "595e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "\n",
    "        model.eval()\n",
    "        generated = x.clone()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            in_seq = generated[:, -context_size:]\n",
    "            logs, _ = self(in_seq, None)\n",
    "            logs = logs[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logs, dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, new_token), dim=1)\n",
    "\n",
    "        model.train()\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504f763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
