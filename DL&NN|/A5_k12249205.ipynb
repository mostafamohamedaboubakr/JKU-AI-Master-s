{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUFNk0BJAMZH"
   },
   "source": [
    "# Assignment 5: Extended Long Short-Term Memory (xLSTM)\n",
    "\n",
    "*Author:* Philipp Seidl\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment, we will explore the xLSTM architecture, a novel extension of the classic LSTM model. The paper can be found here: https://arxiv.org/abs/2405.04517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBfgx3oEAc3W"
   },
   "source": [
    "## Background\n",
    "Recurrent Neural Networks (RNNs), particularly LSTMs, have proven highly effective in various sequence modeling tasks. However, the emergence of Transformers, with their parallel processing capabilities, has shifted the focus away from LSTMs, especially in large-scale language modeling.\n",
    "The xLSTM architecture aims to bridge this gap by enhancing LSTMs with mechanisms inspired by modern LLMs (e.g. block-strucutre, residual connections, ...).  Further it introduces:\n",
    "- Exponential gating with normalization and stabilization techniques, which improves gradient flow and memory capacity.\n",
    "- Modifications to the LSTM memory structure, resulting in two variants:\n",
    "    - sLSTM: Employs a scalar memory with a scalar update rule and a new memory mixing technique through recurrent connections.\n",
    "    - mLSTM: Features a matrix memory, employs a covariance update rule, and is fully parallelizable, making it suitable for scaling.\n",
    "\n",
    "By integrating these extensions into residual block backbones, xLSTM blocks are formed, which can then be residually stacked to create complete xLSTM architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08ut_E9kAdpU"
   },
   "source": [
    "## Exercise 1: Environment Setup\n",
    "\n",
    "When working with new architectures or specialized frameworks, it's essential to correctly set up the environment to ensure reproducability. This exercise focuses on setting up the environment for working with the `xlstm` repository.\n",
    "\n",
    "1. Visit and clone the official repository: [https://github.com/NX-AI/xlstm](https://github.com/NX-AI/xlstm).  \n",
    "2. Set up the environment  \n",
    "3. Document your setup:  \n",
    "   - OS, Python version, Environment setup, CUDA version (if applicable), and GPU details.  \n",
    "   - Note any challenges you faced and how you resolved them. \n",
    "4. Submit your setup as a bash script using the IPython `%%bash` magic. Ensure it is reproducible.\n",
    "\n",
    "Getting mLSTM working only is fine (if you encounter issues with sLSTM cuda kernels)\n",
    "\n",
    "> **Note**: Depending on your system setup, you may need to adjust the `environment_pt220cu121.yaml` file, such as for the CUDA version. For this assignment, it is recommended to run it on GPUs. If you don't have one, consider using  [Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) or other online resources.\n",
    "\n",
    "> **Recommendations**: While the repository suggests using `conda`, we recommend using `mamba` or `micromamba` instead (way faster) (except if you are using colab). Learn more about them here: [https://mamba.readthedocs.io/en/latest/index.html](https://mamba.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Bdb5fIMaKea1",
    "outputId": "fbea5037-812c-41a1-bd42-79f4b9790cd5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "########## SOLUTION BEGIN ##########\n",
    "# first i had to check my setup capabilities\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version (used by PyTorch):\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA capability of the device:\", torch.cuda.get_device_capability())\n",
    "# then i cloned the xlstn repo\n",
    "!git clone https://github.com/NX-AI/xlstm.git\n",
    "\n",
    "# then i had to edit the environment file (environment_pt220cu121.yaml) to fit my setup\n",
    "# then i have created the invironment\n",
    "!conda env create -f environment_pt220cu121.yaml\n",
    "\n",
    "# installing xlstm mlstm pchgs\n",
    "!pip install xlstm\n",
    "!pip install mlstm_kernels\n",
    "\n",
    "#activatinf the xlstm env\n",
    "!conda activate xlstm\n",
    "\n",
    "\n",
    "# problems:\n",
    "\n",
    "# i had many problems creating the environment, till i figured out\n",
    "# that i have ti edit the yaml file to adjust the cuda, pytorch, python, jupyter versions\n",
    "# and removing the un nessesary requirments for my setup\n",
    "\n",
    "#also i havd to change the xlstm file to Xlstm also in the import line because it was calling the wrong _int_.py file \n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "4FejZLBoK_Lo",
    "outputId": "37eadcd9-3134-45df-da5c-0db1a267f332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify your installation of xLSTM:\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "from Xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "use_slstm_kernels = True # set to True if you want to check if sLSTM cuda kernels are working\n",
    "\n",
    "xlstm_cfg = f\"\"\"\n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: {'cuda' if use_slstm_kernels else 'vanilla'}\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 32\n",
    "num_blocks: 7\n",
    "embedding_dim: 64\n",
    "slstm_at: [] # empty = mLSTM only\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "xlstm_stack = xLSTMBlockStack(cfg)\n",
    "\n",
    "x = torch.randn(4, 32, 64).to(DEVICE)\n",
    "xlstm_stack = xlstm_stack.to(DEVICE)\n",
    "y = xlstm_stack(x)\n",
    "y.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbkUQdktAkeG"
   },
   "source": [
    "## Exercise 2: Understanding xLSTM Hyperparameters\n",
    "Explain key hyperparameters that influence the performance and behavior of the xLSTM architecture and explain how they influence total parameter count.\n",
    "The explanation should include: proj_factor, num_heads, act_fn, context_length, num_blocks, embedding_dim, hidden_size, dropout, slstm_at, qkv_proj_blocksize, conv1d_kernel_size. Also include how the matrix memory size of mLSTM is determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "the hyperparameters of xlstm are directly affecting the performance, efficiency, and computational capabiliyt of the model as well as the total parameter count.\n",
    "\n",
    "regarding the efficiency, performance, and parameter count of the model:\n",
    "\n",
    " - for (proj_factor), reducing it reduces the computational cost and increasing it increases the model size so more computations, and it reduces the parameter count.\n",
    "\n",
    " - for (num_heads) increasing it improves the performance but increases memory usage and computational requirements, it increases the parameter count.\n",
    " \n",
    " - for (act_fn) the type of the activation function used affects the convergence and the computational costs, and it has no effect on parameter count\n",
    " \n",
    " - for (contex_length) increasing it is good for capturing temporal dependancies but that will requir more memory and computational requirements, it affects the parameter count but indirectly.\n",
    "\n",
    " - for (num_blocks) increasing the blocks reflects on the performance in agood way but also can add more computational requirements, and the parameters increase linearly with increasing blocks.\n",
    "\n",
    " - for (embedding_dim) increasing it does not realy affect the performance but it can improve the performance, and it increase the parameter count.\n",
    "\n",
    " - for (hidden_size) increasing it improves the model ability to learn compelx patterns but needs large memory size, and it increases the parameters.\n",
    "\n",
    " - for (dropout)it improves generalization but can slow down convergence, and it has no effect on parameter count. \n",
    "\n",
    " - for (slstm_at) usinf sLSTM reduces the number of activation neurons also reduces the memory usage, and it has no effect on patameter counts.\n",
    "\n",
    " - for (qkv_blocksize) reducing it may limit the ability of the model to learn. and it increases the parameter count. \n",
    "\n",
    " - for (conv1d_kernel_size) increasing it can improve performance but also increases computational cost, and it increases the parameter count.\n",
    "\n",
    " - for (matrix memory size in mLSTM) large memory size allows th model to store more contextual info but increases the memory usage, and it has no effect on parameter count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4rSyOdnAv6r"
   },
   "source": [
    "## Exercise 3: Train an xLSTM model on the Trump Dataset from the previous exercise\n",
    "Your task is to train an xLSTM model on the Trump Dataset from the previous exercise. \n",
    "- The goal is to achieve an average validation loss $\\mathcal{L}_{\\text{val}} < 1.35$. \n",
    "- You do not need to perform an extensive hyperparameter search, but you should document your runs. Log your runs with used hyperparameters using tools like wandb, neptune, mlflow, ... or a similar setup. Log training/validation loss and learning rate over steps as well as total trainable parameters of the model for each run.\n",
    "- You can use the training setup from the previous exercises or any setup of your choice using high level training libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVADqjO1A9kI"
   },
   "source": [
    "## Exercise 4: Utilizing a Pretrained Model (Bonus)\n",
    "\n",
    "Foundation Models, those pretrained on large amounts of data are more and more important. We can use those models and fine-tune them on our dataset, rather then training them from scratch.\n",
    "Here are the things to consider:\n",
    "\n",
    "- Model Selection: Choose a pretrained language model from an online repository. Hint: You can explore platforms like Hugging Face (huggingface.co), which host numerous pretrained models.\n",
    "\n",
    "- Dataset: Use the Trump dataset with the same training and validation split as in previous exercises. You do not need to use character tokenization.\n",
    "\n",
    "- Performance Evaluation: Evaluate the performance of the pretrained model on the validation set before and during fine-tuning. Report average-CE-loss as well as an example generated sequence with the same prompt for each epoch.\n",
    " \n",
    "- Fine-tuning: Adjust the learning rate, potentially freeze some layers, train for a few epochs with a framework of your choice (e.g. [lightning](https://lightning.ai/docs/pytorch/stable/), [huggingface](https://huggingface.co/models), ...)\n",
    "\n",
    "- Computational Resources: Be mindful of the computational demands of pretrained models. You might need access to GPUs. Try to keep the model size at a minimum and go for e.g. distilled versions or other small LMs\n",
    "\n",
    "- Hyperparameter Tuning: You can experiment with different learning rates and potentially other hyperparameters during fine-tuning but no need to do this in depth\n",
    "\n",
    "By completing this exercise, you will gain experience with utilizing pretrained models, understanding their capabilities, and the process of fine-tuning. Decreasing the validation loss can be seen a success for this exercise.\n",
    "\n",
    "> **Note**: This is a standalone exercise and doesn't build upon the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqv4tH69Ab0X"
   },
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (xlstm)",
   "language": "python",
   "name": "xlstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
