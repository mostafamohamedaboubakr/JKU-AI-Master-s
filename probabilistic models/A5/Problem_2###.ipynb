{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae711d2-2958-48fe-982b-0c347b0e694a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ea0bf6d9fcdf12a5a344b080fee84b3",
     "grade": false,
     "grade_id": "cell-99662478bf8d98c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import plot_states, get_weather_example, HMM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae26447-5506-404a-8d80-1cc0afbfa899",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eae03af190a3a7a23aae056de8919953",
     "grade": false,
     "grade_id": "cell-534cbc9dff6c7c5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Learning HMMs from Observation Sequences\n",
    "\n",
    "In **Problem 1**, we explored HMM inference algorithms in detail. To test these algorithms, we worked with fully defined HMMs, where the parameters — $\\mathbf{A}$ (transition probabilities), $\\mathbf{B}$ (observation probabilities) and $\\mathbf{\\Pi}$ (initial state probabilities) — were known.\n",
    "\n",
    "However, in real-world scenarios, we often lack direct access to these parameters. Instead, we only observe sequences of outputs, leaving the hidden system dynamics and observation dependencies to be inferred. In this notebook, we will explore the **Baum-Welch Algorithm** for learning HMM parameters from example observation sequences. \n",
    "\n",
    "To get the most out of this exercise, review the lecture slides on *Learning* (SD 6a, p. 54-68) carefully before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb1544-ecdb-493f-bdaf-f2702a635800",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afdcb32b5e712a0292ea49a866398e6a",
     "grade": false,
     "grade_id": "cell-25f2d3846ab4c8ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Likelihood forward and backward messages\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Implement the algorithms to compute the <i>likelihood forward messages</i> and the <i>backward messages</i>. (1 point)\n",
    "</div>\n",
    "\n",
    "To compute the expected values of the hidden counts (SD 6a, p. 62), we need the **likelihood forward message** and the **backward messages**.\n",
    "\n",
    "- The likelihood forward message is calculated as: $$P(S^{(t+1)}, \\mathbf{o}^{(1:t+1)}) = \\mathsf{lf}^{(1:t+1)} = \\mathbf{B}[o^{(t+1)}] \\circ \\mathbf{A}^T \\mathsf{lf}^{(1:t)}$$\n",
    "$\\mathsf{lf}^{(1:t)}_i$: the likelihood of observing $\\mathbf{o}^{(1:t)}$ and ending up in state $s^{t}_i$\n",
    "- The backward message is given by:\n",
    "$$ P(\\mathbf{o}^{(t+1:T)} \\mid S^{(t)}) = \\mathsf{b}^{(t+1:T)} = \\mathbf{A}(\\mathbf{B}[o^{(t+1)}]^T \\circ \\mathsf{b}^{(t+2:T)})$$\n",
    "$\\mathsf{b}^{(t+1:T)}_i$: the probability of observing $\\mathbf{o}^{(t+1:T)}$, given that we are in $s^{t}_i$\n",
    "\n",
    "*Hints*:\n",
    "- Reuse your implementations of the *forward* and *backward* functions of Problem 1.\n",
    "- Remove normalization (the $\\frac{1}{Z}$ constant) from both *forward* and *backward* functions.\n",
    "- Initialize $\\mathsf{b}^{(T+1:T)}$ as a vector of ones instead of a vector of $\\frac{\\mathbf{1}}{\\mid S \\mid}$.\n",
    "\n",
    "*Note*:\n",
    "- While the Baum-Welch Algorithm often uses normalized forward and backward messages for numerical stability, this exercise focuses on the simplest implementation. For this reason, normalization is omitted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547a4867-0e59-4cbb-a2c7-1880ead01777",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2eeabbd06772fc28d08fcbe06d214580",
     "grade": false,
     "grade_id": "cell-08186ac06bb9df88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def likelihood_forward(hmm: HMM, observations: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the likelihood forward message for a given \n",
    "    Hidden Markov Model and observations.\n",
    "    \n",
    "    :param hmm: HMM datastructure\n",
    "    :param observations: Numpy array containing the observations\n",
    "\n",
    "    :return: likelihood forward message (each row represents a time step)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize empty array for the forward messages\n",
    "    f = np.empty((len(observations) + 1, hmm.num_states))\n",
    "    \n",
    "    # Forward pass\n",
    "    f[0] = hmm.pi\n",
    "    num_obs = len(observations)\n",
    "    for i in range(num_obs):\n",
    "        obs = observations[i]\n",
    "        f[i + 1] = hmm.B[obs] * np.dot(f[i] , hmm.A)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aca77d5-39f2-4e48-8110-1843b394bdfb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ec287302388a7287f80b77bec4309cc",
     "grade": true,
     "grade_id": "cell-7dd3b086db768dfd",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity check using weather example from the lecture slides\n",
    "expected_f = np.array([\n",
    " [0.2, 0.3, 0.5],\n",
    " [0.076, 0.087, 0.104],\n",
    " [0.0291, 0.03416, 0.02468],\n",
    " [0.002094, 0.0095082, 0.0211836]\n",
    "])\n",
    "\n",
    "\n",
    "actual_f = likelihood_forward(*get_weather_example())\n",
    "\n",
    "# check shape\n",
    "assert actual_f.shape == expected_f.shape, f'Shape mismatch, expected {expected_f.shape}, but got {actual_f.shape}.'\n",
    "\n",
    "# check values\n",
    "assert np.allclose(expected_f, actual_f), f'Result mismatch, expected \\n{expected_f} \\nbut got \\n{actual_f}. Double-check that you have removed the normalization.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c98e0dd-00df-49e0-ac71-3ca4373be790",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dea55f1e05dd92186730049c678219a",
     "grade": false,
     "grade_id": "cell-d05b2ba4b22d86c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward(hmm: HMM, observations: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the backward message (unnormalized) for a given \n",
    "    Hidden Markov Model and observations.\n",
    "    \n",
    "    :param hmm: HMM datastructure\n",
    "    :param observations: Numpy array containing the observations\n",
    "\n",
    "    :return: backward message (each row represents a time step)\n",
    "    \"\"\"\n",
    "    # initialize empty array for the backward messages\n",
    "    b = np.empty((len(observations) + 1, hmm.num_states))\n",
    "    \n",
    "    # Backward pass\n",
    "    num_obs = len(observations)\n",
    "    b[num_obs] = np.ones(hmm.num_states)\n",
    "    for i in range(num_obs - 1 , -1 , -1):\n",
    "        obs = observations[i]\n",
    "        b[i] = np.dot(b[i + 1] * hmm.B[obs] , hmm.A.T)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bef8811-2df3-48ad-88b9-d0924d2906c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "584324382d6bf261fa03f3226665ee68",
     "grade": true,
     "grade_id": "cell-7b5012ebf4d5c578",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity check using the weather example\n",
    "\n",
    "expected_f = np.array([\n",
    "    [0.03932, 0.038136, 0.026962],\n",
    "    [0.1316, 0.1286, 0.1115],\n",
    "    [0.31, 0.32, 0.52],\n",
    "    [1.0, 1.0, 1.0]\n",
    "])\n",
    "\n",
    "actual_f = backward(*get_weather_example())\n",
    "\n",
    "# check shape\n",
    "assert actual_f.shape == expected_f.shape,f'Shape mismatch, expected {expected_f.shape}, but got {actual_f.shape}.'\n",
    "# check values\n",
    "assert np.allclose(expected_f, actual_f), f'Result mismatch, expected \\n{expected_f} \\nbut got \\n{actual_f}. Double-check that you have removed the normalization.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ce7c0-8c64-486f-b652-85a861c459ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ac717a15d90e4dafc58d886e0c00191",
     "grade": false,
     "grade_id": "cell-1adde3ef3f820487",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sample observations from an HMM\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Implement the <i>sample_hmm</i> function, which samples observations from an HMM. (2 points)\n",
    "</div>\n",
    "\n",
    "We will use this function to sample observations from a known HMM. We will then use the observations to re-discover the original HMM that has generated our observations.\n",
    "\n",
    "`sample_hmm` takes two parameters:\n",
    "- `hmm`: An instance of the HMM class.\n",
    "- `sequence_length`: The length of the observation sequence to sample.\n",
    "\n",
    "`sample_hmm` must return one object:\n",
    "- a NumPy array of shape `(sequence_length,)` containing a sequence of observations.\n",
    "\n",
    "*Hints*:\n",
    "- Sample states and observations in the following order: $s^{(0)}$, $s^{(1)}$, $o^{(1)}$, $s^{(2)}$, $o^{(2)}$, $s^{(3)}$, ...\n",
    "- Sample the inital state $s^{(0)}$ from `hmm.pi`.\n",
    "- Select the correct row of `hmm.A` based on the current state to sample the next state.\n",
    "- Select the correct column of `hmm.B` based on the current state to sample the observation.\n",
    "- Recall that observations start at $t=1$. There is no observation at $t=0$, so `observations[0]` corresponds to $t=1$.\n",
    "- Use the `sample_categorical()` function to sample from a discrete 1-D probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0efd1e11-b251-4ef3-bc73-be889676d7a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c919d49605abb7e80f7bb49eb19cbdb2",
     "grade": false,
     "grade_id": "cell-13ea5255bdbaedf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sample_categorical in module utils:\n",
      "\n",
      "sample_categorical(dist: numpy.ndarray) -> numpy.int64\n",
      "    Draws a single sample from a categorical distribution.\n",
      "    :param dist: NumPy array listing the probability of each outcome.\n",
      "    :returns: Index of the sampled element.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_categorical\n",
    "help(sample_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6ea29f2-5bc7-45b7-9af1-1130ea930000",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab1f21d7c71761054fa2322be612fc6c",
     "grade": false,
     "grade_id": "cell-a8b0dbcc9a33f0f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_hmm(hmm: HMM, sequence_length: int):\n",
    "    \"\"\"\n",
    "    Sample a sequence of observations from a given HMM.\n",
    "\n",
    "    :param hmm: An instance of the HMM class.\n",
    "    :param sequence_length: Length of the observation sequence to generate.\n",
    "    :return: array of sampled observations (length = sequence_length).\n",
    "    \"\"\"\n",
    "\n",
    "    assert sequence_length >= 1\n",
    "    \n",
    "    # Initialize the state and observation sequences\n",
    "    states = np.zeros(sequence_length + 1, dtype=np.int64)\n",
    "    observations = np.zeros(sequence_length, dtype=np.int64)\n",
    "\n",
    "    # sampling from HMM\n",
    "    states[0] = sample_categorical(hmm.pi)\n",
    "    for i in range(sequence_length):\n",
    "        states[i + 1] = sample_categorical(hmm.A[states[i]])\n",
    "   #     print(f\"Step {i}: State = {states[i]}, Transition Probabilities = {hmm.A[states[i], :]}\")\n",
    "        observation_probabilities = hmm.B[states[i] , :]\n",
    "   #     print(f\"Step {i}: State = {states[i]}, Observation Probabilities = {observation_probabilities}\")\n",
    "        if not np.isclose(observation_probabilities.sum() , 1):\n",
    "            observation_probabilities /= observation_probabilities.sum()\n",
    "        observations[i] = sample_categorical(observation_probabilities)\n",
    "    #    print(f\"Step {i}: Sampled Observation = {observations[i]}\")\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00fb9e74-a7c4-4ec5-8cbd-5f80fb264b63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d7457aa6641da35ea340740b57110ca",
     "grade": true,
     "grade_id": "cell-7b1c7ddeeae977c9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Result mismatch, expected \n[1 0 0 1 0 2 0 1 1 1] \nbut got \n[1 0 0 1 0 2 0 0 0 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m actual\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m10\u001b[39m, ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWrong output shape!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: (10, )\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGiven:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m actual\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint64, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWrong numpy array data type!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: np.int64\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGiven:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misclose(actual, expected)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult mismatch, expected \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Result mismatch, expected \n[1 0 0 1 0 2 0 1 1 1] \nbut got \n[1 0 0 1 0 2 0 0 0 1]."
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "weather_hmm = get_weather_example()[0]\n",
    "np.random.seed(42)\n",
    "expected = np.array([1, 0, 0, 1, 0, 2, 0, 1, 1, 1])\n",
    "actual = sample_hmm(weather_hmm, 10)\n",
    "\n",
    "assert type(actual) == np.ndarray, f'\\nWrong output type!\\nExpected: np.ndarray\\nGiven:\\t  {type(actual)}'\n",
    "assert actual.shape == (10, ), f'\\nWrong output shape!\\nExpected: (10, )\\nGiven:\\t  {actual.shape}'\n",
    "assert actual.dtype == np.int64, f'\\nWrong numpy array data type!\\nExpected: np.int64\\nGiven:\\t  {actual.dtype}'\n",
    "assert np.all(np.isclose(actual, expected)), f'Result mismatch, expected \\n{expected} \\nbut got \\n{actual}.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2ea7b-a6d0-4d79-9204-b32d845ad7c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b62450eaf2a2d5a84d640d7c4298e23",
     "grade": false,
     "grade_id": "cell-d6fa9009e86f999a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Computing the Likelihood of an HMM\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Implement the <i>hmm_log_likelihood</i> function, which computes the log-likelihood of observations under a given HMM. (2 points)\n",
    "</div>\n",
    "\n",
    "We will use this function to assess how well a given HMM fits the observations.\n",
    "\n",
    "`hmm_log_likelihood` takes two parameters:\n",
    "- `hmm`: An instance of the HMM class.\n",
    "- `observations_list`: A list of NumPy arrays, where:\n",
    "  - `len(observations_list) = n_samples` (number of observation sequences).\n",
    "  - Each array is of shape `(sequence_length,)`.\n",
    "\n",
    "`hmm_log_likelihood` must return one number:\n",
    "- A single number representing the sum of the log-likelihoods of all observation sequences under the HMM.\n",
    "\n",
    "*Hints*:\n",
    "- Have a look at the lecture slides (SD 6a, p. 48-53).\n",
    "- Use the `likelihood_forward` function to compute the likelihood for each observation sequence.\n",
    "- For a single sequence, compute: $P(\\mathbf{o}^{(1:T)}) = \\sum_i \\mathsf{lf}_i^{(1:T)}$\n",
    "  where $\\mathsf{lf}_i^{(1:T)}$ is the final likelihood forward message.\n",
    "- Accumulate the log probabilities over all observation sequences by summing the individual $\\log P(\\mathbf{o}^{(1:T)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5410d5e-d2b4-4ca7-9c82-d5363ab3ca61",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89cea6f9c9aa2788929fded719478a61",
     "grade": false,
     "grade_id": "cell-987534b1d979ef57",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hmm_log_likelihood(hmm: HMM, observations_list: list):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of the observations\n",
    "    under the given HMM.\n",
    "    \n",
    "    :param hmm: An instance of the HMM class.\n",
    "    :param observations_list: List of Numpy arrays, each representing an\n",
    "                              observation sequence of shape (sequence_length,).\n",
    "\n",
    "    :return: Log-likelihood of the observation sequences under the HMM.\n",
    "    \"\"\"\n",
    "    sum_log_prob = 0\n",
    "    \n",
    "    # HMM loglikelihood\n",
    "    for observations in observations_list:\n",
    "        f_msg = likelihood_forward(hmm , observations)\n",
    "        full_likel = np.sum(f_msg[-1 , :])\n",
    "        sum_log_prob += np.log(full_likel)\n",
    "    return sum_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9c88e0a-4e82-4af4-bef1-595a65547153",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c971fe528cb0c8575f11d60b304fffa",
     "grade": true,
     "grade_id": "cell-0675d8131bfb4b74",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "weather_hmm = get_weather_example()[0]\n",
    "np.random.seed(42)\n",
    "samples = [\n",
    "    np.array([1, 0, 0, 1, 0, 2, 0, 1, 1, 1]),\n",
    "    np.array([1, 0, 2, 1, 0, 2, 0, 0, 0, 1]),\n",
    "    np.array([1, 0, 1, 1, 1, 1, 2, 2, 2, 1]),\n",
    "]\n",
    "\n",
    "actual = hmm_log_likelihood(weather_hmm, samples)\n",
    "expected = -32.8822\n",
    "assert np.all(np.isclose(expected, actual)), f\"Computed log-likelihood value ({round(actual, 3)}) differs from expected ({round(expected, 3)}).\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1d94a-1844-4240-bab5-1c71b8acea91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afa7f1ece05be79d1687b7db93cc1dcf",
     "grade": false,
     "grade_id": "cell-6e866e68e4b02c3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implement the Baum-Welch algorithm\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Implement the <i>baum_welch</i> function, which learns the parameters of an HMM from given observations via Expectation-Maximization. (3 points)\n",
    "</div>\n",
    "\n",
    "Most of the *baum_welch* function is already implemented for you. Your task is to implement the *Maximization step*, i.e., re-estimating the HMM parameters based on expected counts.\n",
    "\n",
    "*Overview:*\n",
    "- The function contains two nested for-loops:\n",
    "  - The **outer for-loop** completes full Expectation-Maximization cycles.\n",
    "  - The **inner for-loop** iterates over the observation sequences to compute expected counts.\n",
    "- **`state_transition_counts`** ($\\tau^{(t)}(i,j)$): Expected number of transitions from state $s_i$ to $s_j$ at time $t$, aggregated over all sequences.\n",
    "- **`state_observation_counts`** ($\\gamma(o_i, s_j)$): Expected number of times the HMM is in state $s_j$ and produces observation $o_i$, aggregated over all sequences and time steps.\n",
    "\n",
    "`baum_welch` takes four parameters:\n",
    "- `hmm`: An instance of the HMM class.\n",
    "- `observations_list`: A list of NumPy arrays. The list has length `n_samples`. Each array is of shape `(sequence_length,)`.\n",
    "- `max_iter`: Maximum number of Expectation-Maximization cycles to perform.\n",
    "- `tol`: Convergence threshold. If the log-likelihood increase between iterations is less than `tol`, the optimization routine stops and returns the current HMM.\n",
    "\n",
    "`baum_welch` must return one object:\n",
    "- The function returns a new HMM instance (`hmm_new`) with parameters (`pi`, `A`, `B`) that correspond to a local maximum of the likelihood function with respect to `observations_list`.\n",
    "\n",
    "\n",
    "*Your task:*\n",
    "- Use the expected counts to calculate the new parameters: `pi_new`, `A_new`, `B_new`.\n",
    "- Use these parameters to create a new HMM instance (`hmm_new`). We avoid in-place updates to leverage the sanity checks in the HMM class constructor.\n",
    "- Compute the log-likelihood of `observations_list` under `hmm_new` and store it in `current_log_likelihood`.\n",
    "\n",
    "*Hints*:\n",
    "- Have a look at the lecture slides (SD 6a, p. 54-68).\n",
    "- **Interpreting `state_transition_counts` ($\\tau^{(t)}(i,j)$):** the expected number of times that the given HMM transitions from state $s_i$ to $s_j$ at time $t$ when generating all given observation sequences.\n",
    "- **Interpreting `state_observation_counts` ($\\gamma(o_i, s_j)$):** the expected number of times that the given HMM is in state $s_j$ and produces $o_i$, aggregated over all time steps when generating all given observation sequences.\n",
    "- `pi_new`: $\\pi_i = \\dfrac{\\sum_j \\tau^{(0)}(i,j)}{\\sum_{i,j} \\tau^{(0)}(i,j)}$, where normalization is necessary as compared to lecture slides, because `state_transition_counts` is aggregated over all observation sequences\n",
    "- `A_new`:\n",
    "    * Sum `state_transition_counts` over time: $\\tau^*(i, j) = \\sum_t \\tau^{(t)}(i,j)$\n",
    "    * Calculate state transition probabilities: $a_{ij} = P(S'=s_j \\mid S=s_i) = \\dfrac{\\tau^{*}(i,j)}{\\sum_j \\tau^{*}(i,j)}$\n",
    "- `B_new`:\n",
    "    * Calculate observation probabilities: $b_{ij}=P(O = o_i \\mid S = s_j) = \\dfrac{\\gamma(o_i, s_j)}{\\sum_i \\gamma(o_i, s_j)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eaea244c-f14d-4fb7-afe4-9fe3d9003391",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e2a1b6c9d863551a1c5814dcfb69581",
     "grade": false,
     "grade_id": "cell-5bfadaf232055ed3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def baum_welch(hmm: HMM, observations_list: list, max_iter: int = 1000, tol: float = 1e-2):\n",
    "    \"\"\"\n",
    "    Baum-Welch algorithm for estimating the parameters of an HMM.\n",
    "    \n",
    "    :param hmm: An instance of the HMM class with inital parameter values.\n",
    "    :param observations_list: List of observation sequences (list of numpy arrays).\n",
    "    :param max_iter: Maximum number of iterations.\n",
    "    :param tol: Tolerance (log-likelhood difference between two iterations) to check convergence.\n",
    "    \"\"\"\n",
    " \n",
    "    seq_len = len(observations_list[0])\n",
    "    prev_log_likelihood = hmm_log_likelihood(hmm, observations_list) \n",
    "    \n",
    "    for iteration in range(1, max_iter + 1):\n",
    "        # Expectation step: obtain expected counts based on current parameters\n",
    "\n",
    "        # tau (see above) \n",
    "        # dim[0]: time, dim[1]: previous states, dim[2]: next states\n",
    "        state_transition_counts = np.zeros((seq_len, hmm.num_states, hmm.num_states))\n",
    "        # gamma (see above)\n",
    "        # dim[0]: observations, dim[1]: states\n",
    "        state_observation_counts = np.zeros((hmm.B.shape[0], hmm.num_states))\n",
    "\n",
    "        for obs_seq in observations_list:\n",
    "            # Compute likelihood_forward and backward messages\n",
    "            lf = likelihood_forward(hmm, obs_seq)\n",
    "            beta = backward(hmm, obs_seq)\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                # see SD 6a, p. 62\n",
    "                # Note: compared to SD 6a, we ...\n",
    "                #   - index observations with t instead of t+1, as o[0] corresponds to t=1 in our implementation\n",
    "                #   - index backward message with t+1, as beta[t+1] corresponds to b^(t+2:T) \n",
    "                st_counts = lf[t][:, None] * hmm.A * hmm.B[obs_seq[t]][None, :] * beta[t + 1][None, :]\n",
    "                # dim[0]: prev states at time t, dim[1]: next states at time t+1\n",
    "                st_counts = st_counts / np.sum(st_counts)\n",
    "\n",
    "                # accumulate expected counts for each timestep over all observation sequences\n",
    "                state_transition_counts[t] += st_counts\n",
    "                \n",
    "                # pick row corresponding to specific observation and add expected state counts\n",
    "                # > why sum over axis=0? \n",
    "                # - we want to sum over previous states to get counts for next states\n",
    "                # - at t=0, we want to get counts for the states at t=1, as observations start at t=1\n",
    "                state_observation_counts[obs_seq[t]] += st_counts.sum(0)\n",
    "                \n",
    "\n",
    "        # Maximization step: we have seen all observation sequences and calculated the expected counts - \n",
    "        #  time for updating the parameters \n",
    "\n",
    "        # 1. calculate the new parameter values for the HMM\n",
    "        # 2. create a new hmm instance 'hmm_new' with the new parameters \n",
    "        # 3. calculate the log-likelihood of the observation sequences under 'hmm_new'\n",
    "        pi_new, A_new, B_new = None, None, None\n",
    "        hmm_new = None\n",
    "        current_log_likelihood = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # 1. Calculate the new parameter values for the HMM\n",
    "        pi_new = state_observation_counts[0] / np.sum(state_observation_counts[0])\n",
    "        A_new = np.sum(state_transition_counts , axis = 0) / np.sum(state_observation_counts , axis = 0)[: , None]\n",
    "        A_new = A_new / A_new.sum(axis = 1 , keepdims = True)\n",
    "        # 2. Create a new HMM instance with the updated parameters\n",
    "        hmm_new = HMM(pi = pi_new , A = A_new , B = B_new)\n",
    "        # 3. Calculate the log-likelihood of the observation sequences under 'hmm_new'\n",
    "        current_log_likelihood = hmm_log_likelihood(hmm_new , observations_list)\n",
    "\n",
    "        assert all([p is not None for p in (pi_new, A_new, B_new, hmm_new, current_log_likelihood)])\n",
    "        hmm = hmm_new\n",
    "        \n",
    "        # Check for convergence\n",
    "        if current_log_likelihood - prev_log_likelihood < tol:\n",
    "            break\n",
    "\n",
    "        # Print intermediate log-likelihood results\n",
    "        if iteration % 30 == 0:\n",
    "            print(f\"Iteration: {iteration}, Log-Likelihood: {round(current_log_likelihood, 3)}\")\n",
    "            \n",
    "        prev_log_likelihood = current_log_likelihood\n",
    "\n",
    "    print(f\"Stopping at Iteration: {iteration}, Final Log-Likelihood: {round(current_log_likelihood, 3)}\")\n",
    "    return hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "903df3a8-caaf-44ee-bef6-224c9baf6b99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa6bea232fcf6b91ce2a016e969496b4",
     "grade": true,
     "grade_id": "cell-6217943e5fba3d36",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 31\u001b[0m\n\u001b[1;32m     23\u001b[0m B \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     [\u001b[38;5;241m0.64336882\u001b[39m, \u001b[38;5;241m0.15303467\u001b[39m, \u001b[38;5;241m0.2408998\u001b[39m],\n\u001b[1;32m     25\u001b[0m     [\u001b[38;5;241m0.29500561\u001b[39m, \u001b[38;5;241m0.70309103\u001b[39m, \u001b[38;5;241m0.3615647\u001b[39m],\n\u001b[1;32m     26\u001b[0m     [\u001b[38;5;241m0.06162557\u001b[39m, \u001b[38;5;241m0.1438743\u001b[39m, \u001b[38;5;241m0.3975355\u001b[39m]\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     29\u001b[0m pi \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.21192234\u001b[39m, \u001b[38;5;241m0.434168\u001b[39m, \u001b[38;5;241m0.35390966\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m estimated_hmm \u001b[38;5;241m=\u001b[39m baum_welch(hmm, observations, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misclose(pi, estimated_hmm\u001b[38;5;241m.\u001b[39mpi)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimated_hmm.pi not correct. Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_hmm\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misclose(A, estimated_hmm\u001b[38;5;241m.\u001b[39mA)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimated_hmm.A not correct. Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_hmm\u001b[38;5;241m.\u001b[39mA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[78], line 64\u001b[0m, in \u001b[0;36mbaum_welch\u001b[0;34m(hmm, observations_list, max_iter, tol)\u001b[0m\n\u001b[1;32m     62\u001b[0m A_new \u001b[38;5;241m=\u001b[39m A_new \u001b[38;5;241m/\u001b[39m A_new\u001b[38;5;241m.\u001b[39msum(axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m , keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# 2. Create a new HMM instance with the updated parameters\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m hmm_new \u001b[38;5;241m=\u001b[39m HMM(pi \u001b[38;5;241m=\u001b[39m pi_new , A \u001b[38;5;241m=\u001b[39m A_new , B \u001b[38;5;241m=\u001b[39m B_new)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 3. Calculate the log-likelihood of the observation sequences under 'hmm_new'\u001b[39;00m\n\u001b[1;32m     66\u001b[0m current_log_likelihood \u001b[38;5;241m=\u001b[39m hmm_log_likelihood(hmm_new , observations_list)\n",
      "File \u001b[0;32m~/Desktop/Probalistic models/Problem set 5/K12341159/utils.py:20\u001b[0m, in \u001b[0;36mHMM.__init__\u001b[0;34m(self, pi, A, B)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (num_states, num_states)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(A\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m B\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m num_states\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(B\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states \u001b[38;5;241m=\u001b[39m num_states\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "np.random.seed(42)\n",
    "num_states, num_obs, alpha = 3, 3, 1\n",
    "pi_init = np.random.dirichlet(np.ones(num_states) * alpha)\n",
    "A_init = np.random.dirichlet(np.ones(num_states) * alpha, size=num_states)\n",
    "B_init = np.random.dirichlet(np.ones(num_obs) * alpha, size=num_states).T\n",
    "hmm = HMM(pi_init, A_init, B_init)\n",
    "\n",
    "observations = [\n",
    "    np.array([1, 0, 0, 1, 0, 2, 0, 1, 1, 1]),\n",
    "    np.array([1, 0, 2, 1, 0, 2, 0, 0, 0, 1]),\n",
    "    np.array([1, 0, 1, 1, 1, 1, 2, 2, 2, 1]),\n",
    "    np.array([1, 0, 1, 1, 1, 1, 2, 2, 1, 0]),\n",
    "    np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 0]),\n",
    "]\n",
    "\n",
    "A = [\n",
    "    [0.62273186, 0.27725238, 0.10001576],\n",
    "    [0.01724652, 0.74786591, 0.23488757],\n",
    "    [0.26978205, 0.00611935, 0.7240986]\n",
    "]\n",
    "\n",
    "B = [\n",
    "    [0.64336882, 0.15303467, 0.2408998],\n",
    "    [0.29500561, 0.70309103, 0.3615647],\n",
    "    [0.06162557, 0.1438743, 0.3975355]\n",
    "]\n",
    "\n",
    "pi = [0.21192234, 0.434168, 0.35390966]\n",
    "\n",
    "estimated_hmm = baum_welch(hmm, observations, max_iter=3)\n",
    "assert np.all(np.isclose(pi, estimated_hmm.pi)), f\"estimated_hmm.pi not correct. Expected: {pi}, Actual: {estimated_hmm.pi}\"\n",
    "assert np.all(np.isclose(A, estimated_hmm.A)), f\"estimated_hmm.A not correct. Expected: {A}, Actual: {estimated_hmm.A}\"\n",
    "assert np.all(np.isclose(B, estimated_hmm.B)), f\"estimated_hmm.B not correct. Expected: {B}, Actual: {estimated_hmm.B}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fc67e-6799-44f3-9d3a-ee1bd9d9c322",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "653f6824e05c65da13bc53cae0db302f",
     "grade": false,
     "grade_id": "cell-f7c9b110313b5c2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Back to Santa\n",
    "\n",
    "In the previous notebook, we assumed that Santa's HMM was magically handed to us, complete with the correct values for $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{\\Pi}$. But fear not—there’s no reason for your 8-year-old you to give up hope! As a budding genius, you already know about the Baum-Welch algorithm for learning HMMs.\n",
    "\n",
    "Determined to uncover the truth about Santa, you prepare a detailed questionnaire for your elementary school classmates. The task? Carefully document everything they heard on Christmas Eve night. By the end of the day, you’ve gathered a large collection of observation sequences—ordered lists of sounds like **silence**, **rumbling**, and **crackle**. Armed with this data, you’re ready to use the Baum-Welch algorithm to unveil Santa’s hidden patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8205bdf3-b722-4521-9042-d0a76d12fd65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a51d87b343b081afba5f48886164d16",
     "grade": false,
     "grade_id": "cell-c0c0e30b88839add",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m  \u001b[38;5;66;03m# Each student recalls a sequence of 30 observations\u001b[39;00m\n\u001b[1;32m     19\u001b[0m students \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# It’s a massive school, full of keen Santa enthusiasts!\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m obs_sequences \u001b[38;5;241m=\u001b[39m [sample_hmm(true_santa_hmm, seq_len) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(students)]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Split data into \"train\" and \"test\" sets (because even the 8-year-old knows about the importance of generalization!)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train_obs \u001b[38;5;241m=\u001b[39m obs_sequences[:\u001b[38;5;241m500\u001b[39m]\n",
      "Cell \u001b[0;32mIn[49], line 21\u001b[0m, in \u001b[0;36msample_hmm\u001b[0;34m(hmm, sequence_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m      states[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_categorical(hmm\u001b[38;5;241m.\u001b[39mA[states[i]])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     print(f\"Step {i}: State = {states[i]}, Transition Probabilities = {hmm.A[states[i], :]}\")\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m      observation_probabilities \u001b[38;5;241m=\u001b[39m hmm\u001b[38;5;241m.\u001b[39mB[states[i] , :]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     print(f\"Step {i}: State = {states[i]}, Observation Probabilities = {observation_probabilities}\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(observation_probabilities\u001b[38;5;241m.\u001b[39msum() , \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# Nothing for you to do here; the 8-year-old you is busy interrogating classmates about mysterious Christmas noises!\n",
    "\n",
    "# Define the true Santa HMM from Problem 1\n",
    "pi = np.array([0.8, 0.1, 0.1, 0.0])\n",
    "\n",
    "A = np.array([[0.7, 0.3, 0.0, 0.0],\n",
    "           [0.3, 0.4, 0.3, 0.0],\n",
    "           [0.0, 0.4, 0.2, 0.4],\n",
    "           [0.0, 0.0, 0.3, 0.7]])\n",
    "\n",
    "B = np.array([[0.8, 0.1, 0.1, 0.1],\n",
    "           [0.1, 0.5, 0.8, 0.3],\n",
    "           [0.1, 0.4, 0.1, 0.6]])\n",
    "\n",
    "true_santa_hmm = HMM(pi, A, B)\n",
    "\n",
    "# Interrogating your classmates...\n",
    "seq_len = 30  # Each student recalls a sequence of 30 observations\n",
    "students = 1000  # It’s a massive school, full of keen Santa enthusiasts!\n",
    "obs_sequences = [sample_hmm(true_santa_hmm, seq_len) for _ in range(students)]\n",
    "\n",
    "# Split data into \"train\" and \"test\" sets (because even the 8-year-old knows about the importance of generalization!)\n",
    "train_obs = obs_sequences[:500]\n",
    "test_obs = obs_sequences[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b0d4a-0722-452a-84f9-1da6c7f8d2d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a84ff47c64e6705c529b8e5f707d12fb",
     "grade": false,
     "grade_id": "cell-f4b24c752ad1b504",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With 500 training (`train_obs`) and 500 testing (`test_obs`) observation sequences, each of length 30, the 8-year-old you sets out to estimate the parameters of the Santa HMM. Naturally, you initialize these parameters using a Dirichlet distribution because you know this often leads to better convergence than a uniform distribution. By breaking the symmetry inherent in uniform initialization, the Dirichlet approach likely leads to the algorithm explores a more diverse range of potential solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89162f24-6853-4908-a0ed-8edaa8067282",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9515ce3d040e8597f8e05b81fcb26c17",
     "grade": false,
     "grade_id": "cell-ecf92fd9fb4c3f20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Nothing for you to do here; the 8-year-old you is busy pretending to be a Bayesian wizard\n",
    "\n",
    "num_states = len(pi)  # Santa's mysterious states\n",
    "num_obs = len(B)      # The noises of Christmas (silence, rumbling, crackle)\n",
    "alpha = 1             # Dirichlet concentration parameter: the higher the value, the more uniform the distribution\n",
    "\n",
    "# Random initialization\n",
    "np.random.seed(3)\n",
    "pi_init = np.random.dirichlet(np.ones(num_states) * alpha)\n",
    "A_init = np.random.dirichlet(np.ones(num_states) * alpha, size=num_states)\n",
    "B_init = np.random.dirichlet(np.ones(num_obs) * alpha, size=num_states).T\n",
    "\n",
    "# Create an HMM\n",
    "initial_hmm = HMM(pi_init, A_init, B_init)\n",
    "\n",
    "# Run Baum-Welch\n",
    "estimated_santa_hmm = baum_welch(initial_hmm, train_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999589cc-fcfe-459a-bf98-42756b7e0b84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "543ce4044ae4303cc328374a99ed1a35",
     "grade": false,
     "grade_id": "cell-1df1388f0a1e987c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "estimated_train_ll = hmm_log_likelihood(estimated_santa_hmm, train_obs)\n",
    "estimated_test_ll = hmm_log_likelihood(estimated_santa_hmm, test_obs)\n",
    "true_train_ll = hmm_log_likelihood(true_santa_hmm, train_obs)\n",
    "true_test_ll = hmm_log_likelihood(true_santa_hmm, test_obs)\n",
    "\n",
    "print(\"Train Log-Likelihood of true santa hmm: \", round(true_train_ll, 3))\n",
    "print(\"Train Log-Likelihood of estimated santa hmm: \", round(estimated_train_ll, 3))\n",
    "\n",
    "print(\"Test Log-Likelihood of true santa hmm: \", round(true_test_ll, 3))\n",
    "print(\"Test Log-Likelihood of estimated santa hmm: \", round(estimated_test_ll, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01e9d4-c1b6-43f9-9c00-1637c9a0ebda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ad740e44608eec32164624c78b6f3a4",
     "grade": false,
     "grade_id": "cell-76634814b519c044",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Copy your implementation of the Viterbi algorithm to the cell below. \n",
    "</div>\n",
    "\n",
    "To visually verify your implementations, we’ll compare the true Santa HMM with the re-estimated Santa HMM by decoding an observation sequence using the Viterbi algorithm. The goal is to check whether both models predict similar state paths, giving us confidence in the accuracy of the re-estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e60328-4bd1-4bef-a0ee-9f03d75f70e4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5607ef141bf834b92084d234f9e4a9ae",
     "grade": false,
     "grade_id": "cell-a3ce3c5af1b1f841",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(hmm: HMM, observations: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the most probable state sequence for a given\n",
    "    Hidden Markov Model and observations.\n",
    "    \n",
    "    :param hmm: HMM data structure\n",
    "    :param observations: Numpy array containing the observations\n",
    "\n",
    "    :return: Viterbi messages (each row represents a time step) and\n",
    "             Numpy array of state IDs representing the most probable state sequence\n",
    "    \"\"\"\n",
    "    v = np.empty((len(observations) + 1, hmm.num_states))\n",
    "    bp = np.empty_like(v, dtype=int)\n",
    "    p = np.empty((len(observations) + 1,), dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return v, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efbe16-cf1f-4272-8f51-697667cf706a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a288e9c517767f6f378cca587e3aa2d4",
     "grade": false,
     "grade_id": "cell-f73aa7aa697a1e30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "obs = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "estimated_vt, estimated_vtp  = viterbi(estimated_santa_hmm, obs)\n",
    "true_vt, true_vtp  = viterbi(true_santa_hmm, obs)\n",
    "plot_states(true_vtp, true_vt, 'Viterbi using true Santa HMM')\n",
    "plot_states(estimated_vtp, estimated_vt, 'Viterbi using estimated Santa HMM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715edce-3362-4a74-9c14-72552fc51db1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2aeb5b2c567699540f38c82f23a22c4",
     "grade": false,
     "grade_id": "cell-69d486add7ecbcb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"> Identify the remaining inconsistency in our estimated Santa HMM and resolve it by storing the correct value in the variable <i>perm</i>. (1 point) </div>\n",
    "\n",
    "Oops, something doesn’t seem quite right. The Baum-Welch algorithm looks solid: the log-likelihood with respect to the training observations increases monotonically in every EM iteration, and the training and test log-likelihoods compared to those of the true Santa HMM seem very reasonable. \n",
    "\n",
    "We would expect the paths found via Viterbi decoding to differ slightly between the estimated and true Santa HMM due to some variability. But here’s the catch: the paths differ far more than expected. What might be causing this inconsistency in the path found by the estimated Santa HMM?\n",
    "\n",
    "If your solution is correct, plotting the two paths again in the code cell below should result in similar paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d884a-a6a3-4b78-8313-55aed3aa038c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fed787736054b543221fe20c9f521f0",
     "grade": false,
     "grade_id": "cell-8a167adedebb8b30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "perm = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816868b-f02c-4b6c-bb74-bf532fd8a349",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c20bfcee7778361140ba5eb70afa9ca",
     "grade": true,
     "grade_id": "cell-46019a40c52d8312",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "assert perm is not None, \"You must define the variable 'perm'!\"\n",
    "assert isinstance(perm, Iterable), \"'perm' must be an iterable!\"\n",
    "assert len(perm) == 4, \"'perm' must contain exactly 4 elements (one for each state)!\"\n",
    "assert all([isinstance(elem, int) for elem in perm]), \"the 4 elements of 'perm' must be integers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67d7c9-fc88-49dc-90d7-51b8f9d07cac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc53822ce774c57dd23008039ec2350f",
     "grade": false,
     "grade_id": "cell-2ecad55b1a46d7d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pi_aligned = estimated_santa_hmm.pi[perm]\n",
    "A_aligned = estimated_santa_hmm.A[perm][:, perm]\n",
    "B_aligned = estimated_santa_hmm.B[:, perm]\n",
    "aligned_santa_hmm = HMM(pi_aligned, A_aligned, B_aligned)\n",
    "\n",
    "estimated_vt, estimated_vtp  = viterbi(aligned_santa_hmm, obs)\n",
    "true_vt, true_vtp  = viterbi(true_santa_hmm, obs)\n",
    "plot_states(true_vtp, true_vt, 'Viterbi using True Santa HMM')\n",
    "plot_states(estimated_vtp, estimated_vt, 'Viterbi using estimated Santa HMM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
